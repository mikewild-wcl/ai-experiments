#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"name":"csharp","languageName":"csharp"},{"name":"fsharp","languageName":"F#","aliases":["f#","fs"]},{"name":"html","languageName":"HTML"},{"name":"http","languageName":"HTTP"},{"name":"javascript","languageName":"JavaScript","aliases":["js"]},{"name":"mermaid","languageName":"Mermaid"},{"name":"pwsh","languageName":"PowerShell","aliases":["powershell"]},{"name":"value"}]}}

#!markdown

# Semantic Kernel prompt templates

Based on chapter 4 of **Building effective LLM-based applications with Semantic Kernel**. The original code is in https://github.com/wmeints/effective-llm-applications/tree/main/samples/chapter-04.

#!csharp

#r "nuget:Azure.Identity"
#r "nuget:Microsoft.SemanticKernel"
#r "nuget:Microsoft.SemanticKernel.PromptTemplates.Handlebars"
#r "nuget:Microsoft.SemanticKernel.Yaml"

#!csharp

using Azure.AI.OpenAI;
using Azure.Identity;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.AzureOpenAI;
using Microsoft.SemanticKernel.PromptTemplates.Handlebars;
using System.Web;

const string modelsEndpoint = "https://mw-models-2.openai.azure.com/";
const string deploymentName = "gpt-4.1-mini";
const string embeddingDeploymentName = "text-embedding-3-small";

#!csharp

var kernelBuilder = Kernel.CreateBuilder();

var apiClient = new AzureOpenAIClient(
    new Uri(modelsEndpoint),
     new DefaultAzureCredential());

kernelBuilder.AddAzureOpenAIChatCompletion(
    deploymentName: deploymentName,
    azureOpenAIClient: apiClient
);

var kernel = kernelBuilder.Build();

#!markdown

## Basic prompt template

#!csharp

// Basic prompt template
var promptTemplate = "Help me cook something nice, give me a recipe for {{ $dish }}";

var executionSettings = new AzureOpenAIPromptExecutionSettings
{
    MaxTokens = 500,
    Temperature = 0.5,
    TopP = 1.0,
    FrequencyPenalty = 0.0,
    PresencePenalty = 0.0
};

var result = await kernel.InvokePromptAsync(promptTemplate, new KernelArguments
{
    ["dish"] = "pizza"
});

result.ToString()

#!csharp

## Kernel function invoking template

#!csharp

// Kernel function invoking template
var promptTemplate = """
    Help me cook something nice, give me a recipe for {{ dish }}. Use the ingredients I have in the fridge: 

    {{#each ingredients}}
        {{ . }}
    {{/each}}
    """;

var executionSettings = new AzureOpenAIPromptExecutionSettings
{
    MaxTokens = 2000,
    Temperature = 0.5,
    TopP = 1.0,
    FrequencyPenalty = 0.0,
    PresencePenalty = 0.0    
};

var prompt = kernel.CreateFunctionFromPrompt(
    promptTemplate, templateFormat: "handlebars",
    promptTemplateFactory: new HandlebarsPromptTemplateFactory
    {
        AllowDangerouslySetContent = true
    },
    executionSettings: executionSettings);

/*
Argument 'ingredients' has a value that doesn't support automatic encoding. Set AllowDangerouslySetContent to 'true' for this argument and implement custom encoding, or provide the value as a string.

See this: https://github.com/microsoft/semantic-kernel/pull/12983
          https://devblogs.microsoft.com/semantic-kernel/encoding-changes-for-template-arguments-in-semantic-kernel/
*/
var result = await kernel.InvokeAsync(prompt, new KernelArguments
{
    ["dish"] = "pizza",
     ["ingredients"] = new List<string>
     {
        HttpUtility.HtmlEncode("pepperoni"),
        HttpUtility.HtmlEncode("mozzarella"),
        HttpUtility.HtmlEncode("spinach"),
    }
});

result.ToString()

#!markdown

## Handlebars prompt template

#!csharp

// Handlebars prompt
var promptTemplate = """
Help me cook something nice, give me a recipe for {{ dish }}. Use the ingredients I have in the fridge: 

{{#each ingredients}}
    {{ . }}
{{/each}}
""";

var executionSettings = new AzureOpenAIPromptExecutionSettings
{
    MaxTokens = 500,
    Temperature = 0.5,
    TopP = 1.0,
    FrequencyPenalty = 0.0,
    PresencePenalty = 0.0
};

var result = await kernel.InvokePromptAsync(promptTemplate,
    arguments: new KernelArguments(executionSettings)
    {
        ["dish"] = "pizza",
        ["ingredients"] = new List<string>
        {
             HttpUtility.HtmlEncode("pepperoni"), 
             HttpUtility.HtmlEncode("mozarella"), 
             HttpUtility.HtmlEncode("spinach")
        }
    },
    templateFormat: "handlebars",
    promptTemplateFactory: new HandlebarsPromptTemplateFactory
    {
        AllowDangerouslySetContent = true
    });

result.ToString()    

#!markdown

## YAML prompt template

#!csharp

//YAML template
var promptTemplate = """
name: GenerateRecipe
description: Generates a recipe based on ingredients in your fridge
template: |
  Help me cook something nice, give me a recipe for {{ dish }}. Use the ingredients I have in the fridge: 

  {{#each ingredients}}
      {{ . }}
  {{/each}}
template_format: handlebars
input_variables:
  - name: dish
    description: The name of the dish you want to make
    is_required: true
  - name: ingredients
    description: A list of ingredient names you have in the fridge
    is_required: true
execution_settings:
  default:
    top_p: 0.98
    temperature: 0.7
    presence_penalty: 0.0
    frequency_penalty: 0.0
    max_tokens: 1200
""";

// var promptTemplate = File.ReadAllText(Path.Join(Directory.GetCurrentDirectory(), "prompt.yaml"));
var prompt = kernel.CreateFunctionFromPromptYaml(
    promptTemplate,
    new HandlebarsPromptTemplateFactory
    {
            AllowDangerouslySetContent = true
    });

var result = await kernel.InvokeAsync(prompt,
    arguments: new KernelArguments
    {
        ["dish"] = "pizza",
        ["ingredients"] = new List<string>
        {
            HttpUtility.HtmlEncode("pepperoni"), 
            HttpUtility.HtmlEncode("mozarella"), 
            HttpUtility.HtmlEncode("spinach")
        }
    });

result.ToString()

#!markdown

## Streaming chat completion

#!csharp

// Streaming chat completion
var executionSettings = new AzureOpenAIPromptExecutionSettings()
{
    Temperature = 0.7,
    PresencePenalty = 0.2,
    FrequencyPenalty = 0.4,
    TopP = 0.98,
};

var chatHistory = new ChatHistory();

chatHistory.AddSystemMessage("You're a digital chef help me cook. Your name is Flora.");
chatHistory.AddUserMessage("Hi, I'd like a nice recipe for a french style apple pie");

var chatCompletionService = kernel.Services.GetService<IChatCompletionService>();
var responseIterator = chatCompletionService!.GetStreamingChatMessageContentsAsync(chatHistory);

await foreach (var token in responseIterator)
{
    Console.Write(token.Content);
}

#!markdown

## POML

Look into POML files:

https://techcommunity.microsoft.com/blog/educatordeveloperblog/unlock-the-full-potential-of-llms-with-poml-the-markup-language-for-prompts/4447849
https://github.com/microsoft/poml
